{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d787b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /home/codespace/.python/current/lib/python3.12/site-packages (3.10.0)\n",
      "Requirement already satisfied: absl-py in /home/codespace/.python/current/lib/python3.12/site-packages (from keras) (2.3.0)\n",
      "Requirement already satisfied: numpy in /home/codespace/.python/current/lib/python3.12/site-packages (from keras) (2.3.0)\n",
      "Requirement already satisfied: rich in /home/codespace/.python/current/lib/python3.12/site-packages (from keras) (14.0.0)\n",
      "Requirement already satisfied: namex in /home/codespace/.python/current/lib/python3.12/site-packages (from keras) (0.1.0)\n",
      "Requirement already satisfied: h5py in /home/codespace/.python/current/lib/python3.12/site-packages (from keras) (3.14.0)\n",
      "Requirement already satisfied: optree in /home/codespace/.python/current/lib/python3.12/site-packages (from keras) (0.16.0)\n",
      "Requirement already satisfied: ml-dtypes in /home/codespace/.python/current/lib/python3.12/site-packages (from keras) (0.5.1)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from keras) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/codespace/.local/lib/python3.12/site-packages (from optree->keras) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich->keras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow in /home/codespace/.python/current/lib/python3.12/site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorflow) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/codespace/.local/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from tensorflow) (76.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/codespace/.local/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/codespace/.local/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorflow) (1.73.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorflow) (3.10.0)\n",
      "Collecting numpy<2.2.0,>=1.26.0 (from tensorflow)\n",
      "  Using cached numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /home/codespace/.python/current/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in /home/codespace/.python/current/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /home/codespace/.python/current/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/codespace/.local/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Using cached numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.3.0\n",
      "    Uninstalling numpy-2.3.0:\n",
      "      Successfully uninstalled numpy-2.3.0\n",
      "Successfully installed numpy-2.1.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in /home/codespace/.python/current/lib/python3.12/site-packages (2.1.3)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.3.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Using cached numpy-2.3.0-cp312-cp312-manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.3\n",
      "    Uninstalling numpy-2.1.3:\n",
      "      Successfully uninstalled numpy-2.1.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.3.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 22:51:49.724999: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-12 22:51:54.534564: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-12 22:51:56.048300: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749768719.207076    1969 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749768719.856227    1969 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749768726.921545    1969 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749768726.921585    1969 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749768726.921589    1969 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749768726.921592    1969 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-12 22:52:07.289242: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%pip install keras\n",
    "%pip install tensorflow\n",
    "%pip install --upgrade numpy\n",
    "\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b845ac1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the project gutenberg ebook of trading with mexico\n",
      "    \n",
      "this ebook is for the use of anyone anywhere in the united states and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. you may copy it, give it away or re-use it under the terms\n",
      "of the project gutenberg license included with this ebook or online\n",
      "at www.gutenberg.org. if you are not located in the united states,\n",
      "you will have to check the laws of the country where you are located\n",
      "before using this ebook.\n",
      "\n",
      "title: trading with mexico\n",
      "\n",
      "author: wallace thompson\n",
      "\n",
      "release date: february 26, 2025 [ebook #75469]\n",
      "\n",
      "language: english\n",
      "\n",
      "original publication: new york: dodd, mead and company, 1921\n",
      "\n",
      "credits: the online distributed proofreading team at https://www.pgdp.net (this file was produced from images generously made available by the internet archive)\n",
      "\n",
      "\n",
      "*** start of the project gutenberg ebook trading with mexico ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                              trading with\n",
      "                                 mexico\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = \"/content/Trading with Mexico.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "print(raw_text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3a8f8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = ''.join(c for c in raw_text if not c.isdigit())\n",
    "chars = sorted(list(set(raw_text)))\n",
    "\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars)) \n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3d6c92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Charaters in the text; corpus length:  387019\n",
      "Total Vocab:  63\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Charaters in the text; corpus length: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0d805be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 38696\n"
     ]
    }
   ],
   "source": [
    "seq_length = 60\n",
    "step = 10\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, step):\n",
    "    sentences.append(raw_text[i: i + seq_length])\n",
    "    next_chars.append(raw_text[i + seq_length])\n",
    "\n",
    "n_patterns = len(sentences)\n",
    "print('Number of sequences:', n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6858769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38696, 60, 63)\n",
      "(38696, 63)\n",
      "[[False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False  True False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False]\n",
      " [False  True False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False  True False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False  True False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "   True False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False  True False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "   True False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False  True False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False  True False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False]\n",
      " [False  True False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False]]\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros((len(sentences), seq_length, n_vocab), dtype=np.bool_)\n",
    "y = np.zeros((len(sentences), n_vocab), dtype=np.bool_)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_to_int[char]] = 1\n",
    "    y[i, char_to_int[next_chars[i]]] = 1\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37118d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 22:52:30.302169: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/home/codespace/.python/current/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,304</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,127</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m98,304\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m)             │         \u001b[38;5;34m8,127\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">106,431</span> (415.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m106,431\u001b[0m (415.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">106,431</span> (415.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m106,431\u001b[0m (415.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(seq_length, n_vocab)))\n",
    "model.add(Dense(n_vocab, activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffe711cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 22:52:31.893706: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 146270880 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 2.8013\n",
      "Epoch 1: loss improved from inf to 2.52303, saving model to saved_weights-01-2.5230.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 143ms/step - loss: 2.8004\n",
      "Epoch 2/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 2.2363\n",
      "Epoch 2: loss improved from 2.52303 to 2.18341, saving model to saved_weights-02-2.1834.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 120ms/step - loss: 2.2359\n",
      "Epoch 3/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 1.9317\n",
      "Epoch 3: loss improved from 2.18341 to 1.90648, saving model to saved_weights-03-1.9065.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 95ms/step - loss: 1.9315\n",
      "Epoch 4/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 1.7810\n",
      "Epoch 4: loss improved from 1.90648 to 1.76712, saving model to saved_weights-04-1.7671.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 94ms/step - loss: 1.7809\n",
      "Epoch 5/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 1.6440\n",
      "Epoch 5: loss improved from 1.76712 to 1.65484, saving model to saved_weights-05-1.6548.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 92ms/step - loss: 1.6441\n",
      "Epoch 6/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 1.5541\n",
      "Epoch 6: loss improved from 1.65484 to 1.56500, saving model to saved_weights-06-1.5650.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 93ms/step - loss: 1.5542\n",
      "Epoch 7/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 1.4679\n",
      "Epoch 7: loss improved from 1.56500 to 1.48579, saving model to saved_weights-07-1.4858.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 91ms/step - loss: 1.4680\n",
      "Epoch 8/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 1.3730\n",
      "Epoch 8: loss improved from 1.48579 to 1.41520, saving model to saved_weights-08-1.4152.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 90ms/step - loss: 1.3733\n",
      "Epoch 9/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 1.3248\n",
      "Epoch 9: loss improved from 1.41520 to 1.34951, saving model to saved_weights-09-1.3495.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 91ms/step - loss: 1.3250\n",
      "Epoch 10/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 1.2571\n",
      "Epoch 10: loss improved from 1.34951 to 1.29328, saving model to saved_weights-10-1.2933.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 92ms/step - loss: 1.2573\n",
      "Epoch 11/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 1.2124\n",
      "Epoch 11: loss improved from 1.29328 to 1.24395, saving model to saved_weights-11-1.2439.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 109ms/step - loss: 1.2126\n",
      "Epoch 12/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 1.1691\n",
      "Epoch 12: loss improved from 1.24395 to 1.20452, saving model to saved_weights-12-1.2045.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 92ms/step - loss: 1.1693\n",
      "Epoch 13/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 1.1129\n",
      "Epoch 13: loss improved from 1.20452 to 1.16607, saving model to saved_weights-13-1.1661.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 91ms/step - loss: 1.1132\n",
      "Epoch 14/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 1.1005\n",
      "Epoch 14: loss improved from 1.16607 to 1.13337, saving model to saved_weights-14-1.1334.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 91ms/step - loss: 1.1008\n",
      "Epoch 15/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 1.0599\n",
      "Epoch 15: loss improved from 1.13337 to 1.10720, saving model to saved_weights-15-1.1072.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 92ms/step - loss: 1.0603\n",
      "Epoch 16/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 1.0394\n",
      "Epoch 16: loss improved from 1.10720 to 1.08534, saving model to saved_weights-16-1.0853.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 91ms/step - loss: 1.0397\n",
      "Epoch 17/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 1.0356\n",
      "Epoch 17: loss improved from 1.08534 to 1.07092, saving model to saved_weights-17-1.0709.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 92ms/step - loss: 1.0358\n",
      "Epoch 18/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 1.0099\n",
      "Epoch 18: loss improved from 1.07092 to 1.05176, saving model to saved_weights-18-1.0518.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 91ms/step - loss: 1.0101\n",
      "Epoch 19/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.9954\n",
      "Epoch 19: loss improved from 1.05176 to 1.03475, saving model to saved_weights-19-1.0348.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 91ms/step - loss: 0.9956\n",
      "Epoch 20/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.9664\n",
      "Epoch 20: loss improved from 1.03475 to 1.02400, saving model to saved_weights-20-1.0240.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 91ms/step - loss: 0.9668\n",
      "Epoch 21/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.9673\n",
      "Epoch 21: loss improved from 1.02400 to 1.01674, saving model to saved_weights-21-1.0167.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 92ms/step - loss: 0.9676\n",
      "Epoch 22/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.9528\n",
      "Epoch 22: loss improved from 1.01674 to 1.01126, saving model to saved_weights-22-1.0113.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 91ms/step - loss: 0.9532\n",
      "Epoch 23/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.9500\n",
      "Epoch 23: loss improved from 1.01126 to 1.00306, saving model to saved_weights-23-1.0031.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 91ms/step - loss: 0.9504\n",
      "Epoch 24/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.9527\n",
      "Epoch 24: loss improved from 1.00306 to 0.99531, saving model to saved_weights-24-0.9953.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 93ms/step - loss: 0.9530\n",
      "Epoch 25/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.9334\n",
      "Epoch 25: loss improved from 0.99531 to 0.97884, saving model to saved_weights-25-0.9788.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 91ms/step - loss: 0.9337\n",
      "Epoch 26/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.9126\n",
      "Epoch 26: loss improved from 0.97884 to 0.96374, saving model to saved_weights-26-0.9637.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 91ms/step - loss: 0.9130\n",
      "Epoch 27/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.9096\n",
      "Epoch 27: loss improved from 0.96374 to 0.95881, saving model to saved_weights-27-0.9588.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 92ms/step - loss: 0.9099\n",
      "Epoch 28/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.8940\n",
      "Epoch 28: loss improved from 0.95881 to 0.95280, saving model to saved_weights-28-0.9528.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 91ms/step - loss: 0.8944\n",
      "Epoch 29/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.9003\n",
      "Epoch 29: loss improved from 0.95280 to 0.94401, saving model to saved_weights-29-0.9440.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 91ms/step - loss: 0.9005\n",
      "Epoch 30/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.8832\n",
      "Epoch 30: loss improved from 0.94401 to 0.93193, saving model to saved_weights-30-0.9319.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 92ms/step - loss: 0.8835\n",
      "Epoch 31/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.8846\n",
      "Epoch 31: loss improved from 0.93193 to 0.91941, saving model to saved_weights-31-0.9194.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 90ms/step - loss: 0.8849\n",
      "Epoch 32/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.8785\n",
      "Epoch 32: loss improved from 0.91941 to 0.91642, saving model to saved_weights-32-0.9164.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 91ms/step - loss: 0.8787\n",
      "Epoch 33/50\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.8693\n",
      "Epoch 33: loss improved from 0.91642 to 0.91103, saving model to saved_weights-33-0.9110.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 91ms/step - loss: 0.8694\n",
      "Epoch 34/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.8477\n",
      "Epoch 34: loss improved from 0.91103 to 0.90650, saving model to saved_weights-34-0.9065.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 92ms/step - loss: 0.8481\n",
      "Epoch 35/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.8577\n",
      "Epoch 35: loss improved from 0.90650 to 0.89524, saving model to saved_weights-35-0.8952.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 92ms/step - loss: 0.8580\n",
      "Epoch 36/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.8420\n",
      "Epoch 36: loss improved from 0.89524 to 0.88960, saving model to saved_weights-36-0.8896.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 92ms/step - loss: 0.8424\n",
      "Epoch 37/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.8369\n",
      "Epoch 37: loss improved from 0.88960 to 0.88033, saving model to saved_weights-37-0.8803.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 91ms/step - loss: 0.8372\n",
      "Epoch 38/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.8357\n",
      "Epoch 38: loss improved from 0.88033 to 0.87362, saving model to saved_weights-38-0.8736.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 92ms/step - loss: 0.8360\n",
      "Epoch 39/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.8206\n",
      "Epoch 39: loss improved from 0.87362 to 0.86586, saving model to saved_weights-39-0.8659.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 90ms/step - loss: 0.8209\n",
      "Epoch 40/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.8113\n",
      "Epoch 40: loss improved from 0.86586 to 0.85945, saving model to saved_weights-40-0.8595.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 90ms/step - loss: 0.8116\n",
      "Epoch 41/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.7895\n",
      "Epoch 41: loss improved from 0.85945 to 0.84837, saving model to saved_weights-41-0.8484.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 91ms/step - loss: 0.7899\n",
      "Epoch 42/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.7925\n",
      "Epoch 42: loss improved from 0.84837 to 0.84390, saving model to saved_weights-42-0.8439.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 91ms/step - loss: 0.7928\n",
      "Epoch 43/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.8076\n",
      "Epoch 43: loss did not improve from 0.84390\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 91ms/step - loss: 0.8079\n",
      "Epoch 44/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.8011\n",
      "Epoch 44: loss did not improve from 0.84390\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 90ms/step - loss: 0.8014\n",
      "Epoch 45/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.7856\n",
      "Epoch 45: loss improved from 0.84390 to 0.83657, saving model to saved_weights-45-0.8366.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 90ms/step - loss: 0.7860\n",
      "Epoch 46/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.7792\n",
      "Epoch 46: loss improved from 0.83657 to 0.83304, saving model to saved_weights-46-0.8330.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 90ms/step - loss: 0.7795\n",
      "Epoch 47/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.7709\n",
      "Epoch 47: loss improved from 0.83304 to 0.82459, saving model to saved_weights-47-0.8246.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 90ms/step - loss: 0.7713\n",
      "Epoch 48/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.7720\n",
      "Epoch 48: loss improved from 0.82459 to 0.81848, saving model to saved_weights-48-0.8185.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 89ms/step - loss: 0.7723\n",
      "Epoch 49/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.7649\n",
      "Epoch 49: loss improved from 0.81848 to 0.81287, saving model to saved_weights-49-0.8129.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 90ms/step - loss: 0.7652\n",
      "Epoch 50/50\n",
      "\u001b[1m302/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.7597\n",
      "Epoch 50: loss improved from 0.81287 to 0.80392, saving model to saved_weights-50-0.8039.keras\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 90ms/step - loss: 0.7600\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint \n",
    "\n",
    "filepath = \"saved_weights-{epoch:02d}-{loss:.4f}.keras\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history = model.fit(x, y, \n",
    "                    batch_size=128,\n",
    "                    epochs=50,\n",
    "                    callbacks=callbacks_list)\n",
    "model.save('my_saved_weights_50epochs.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e92be71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# plot the training and validation accuracy and loss at each epoch\n",
    "loss = history.history['loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c494d85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds)\n",
    "    exp_preds = np.exp(preds)  # exp of log(x), isn't this same as x??\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57f124d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Seed for our text prediction: \"ecurity the mexican\n",
      "government’s pledge of a portion of the \"\n",
      "ecurity the mexican\n",
      "government’s pledge of a portion of the "
     ]
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction\n",
    "# load the network weights\n",
    "filename = \"/my_saved_weights_50epochs.keras\"\n",
    "model.load_weights(filename)\n",
    "\n",
    "# Pick a random sentence from the text as seed.\n",
    "start_index = random.randint(0, n_chars - seq_length - 1)\n",
    "\n",
    "# Initiate generated text and keep adding new predictions and print them out\n",
    "generated = ''\n",
    "sentence = raw_text[start_index: start_index + seq_length]\n",
    "generated += sentence\n",
    "\n",
    "print('------ Seed for our text prediction: \"' + sentence + '\"')\n",
    "sys.stdout.write(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee4053c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great madueme in ap yout in a cruszal many and to the “oil from personal sums. the precation of the marget and of labor, shipp pucc, provisorgect of ravine that proboblectity esoorgat we great of eupshall agricut\n",
      "lifence in its ever bone ffield nearly engour on vistrance barkerd. for which is sistern and excractive in the huscussing of ducation wersk and pater, with\n",
      "with with its sife bur to-day d\n"
     ]
    }
   ],
   "source": [
    "for i in range(400):  # Number of characters including spaces\n",
    "    x_pred = np.zeros((1, seq_length, n_vocab))\n",
    "    for t, char in enumerate(sentence):\n",
    "        x_pred[0, t, char_to_int[char]] = 1.\n",
    "\n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    next_index = sample(preds)\n",
    "    next_char = int_to_char[next_index]\n",
    "\n",
    "    generated += next_char\n",
    "    sentence = sentence[1:] + next_char\n",
    "\n",
    "    sys.stdout.write(next_char)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
